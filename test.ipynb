{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grape import grape\n",
    "from grape import algorithms\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deap import creator, base, tools\n",
    "import random\n",
    "\n",
    "from scripts.simulations.util import falsifier, evaluate\n",
    "import multiprocessing\n",
    "\n",
    "import os \n",
    "import mlflow\n",
    "\n",
    "def evaluate_(ind, dummy):\n",
    "\n",
    "    return 10,\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\"\n",
    "\n",
    "# # Set our tracking server uri for logging\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "GRAMMAR_FILE = 'test.bnf'\n",
    "BNF_GRAMMAR = grape.Grammar(r\"./scripts/scenarios/\" + GRAMMAR_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(BNF_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNF_GRAMMAR.n_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \\\n",
    "\"\"\"<rule>                 ::= Ensure safety of the pedestrian and the pet in a scenario where a pedestrian of <pedestrian-ethnicity> <pedestrian-gender> of age <pedestrian-age> <pedestrian-behavior> on the sidewalk trying to <pedestrian-behavior> with a <pet-color> <pet-breed> in the <terrain> during <lighting-condition> on a <weather> day\n",
    "<pedestrian-ethnicity> ::= Asian | Europian | African-American\n",
    "<pedestrian-gender>    ::= Male | Female | Others\n",
    "<pedestrian-age>       ::= below-10 | 10-20 | 20-40 | above-40\n",
    "<pedestrian-behavior>  ::= Walking | Jogging | Running | Cross\n",
    "<pet-color>            ::= Black | White | Grey | Brown\n",
    "<pet-breed>            ::= Dog | Cat | Possum | Rabbit\n",
    "<terrain>              ::= City | Country | Town | Off-road\n",
    "<lighting-condition>   ::= Morning | Noon | Evening | Twilight\n",
    "<weather>              ::= Sunny | Windy | Cloudy | Snowy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_string = re.sub(r\"[^a-zA-Z0-9:<>=\\-\\|\\s]\", \"\", text)\n",
    "updated_string = re.sub(r\"-\", \"_\", allowed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LARK grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: \n",
      "    \"ALWAYS\" Achieve \"Safe Braking\" \n",
      "    by \n",
      "        Achieving \"Pedestrian Check\" using \"Perception Module\"\n",
      "            operationalized as\n",
      "                \"capturing image\" performed by \"camera module\" taking input \"clock tick\" producing output \"image\"\n",
      "                triggering\n",
      "                \"Detect Pedestrian\" performed by \"yolov5m\" \n",
      "                taking input \"image\" producing output \"pedestrian detection confidence\" & \"pedestrian detection flag\"\n",
      "        followed by \n",
      "        Achieving \"braking\" using \"braking module\"\n",
      "            operationalized as \n",
      "                \"Bring down throttle\" & \"Apply Brakes\" if \"pedestrian detection flag=True\" performed by \"Braking Module\" \n",
      "                taking input \"pedestrian detection flag\" producing output \"Braking Status Flag\" achieving \"safe breaking\"\n",
      "    in scenario where\n",
      "        \"A <pedestrian>  wearing a <dress> dress trying to cross road from <direction> at a distance of <distance> on a day with fog density <fog_density>\"\n",
      "    \n",
      "goal\n",
      "  temporal_op\t\"ALWAYS\"\n",
      "  objective\t\"Safe Braking\"\n",
      "  refined_goal\n",
      "    refined_goal\n",
      "      objective\t\"Pedestrian Check\"\n",
      "      agent\t\"Perception Module\"\n",
      "      operation\n",
      "        operation\n",
      "          task\t\"capturing image\"\n",
      "          module\t\"camera module\"\n",
      "          input\t\"clock tick\"\n",
      "          output\t\"image\"\n",
      "        operation\n",
      "          task\t\"Detect Pedestrian\"\n",
      "          module\t\"yolov5m\"\n",
      "          input\t\"image\"\n",
      "          output\t\"pedestrian detection confidence\"\n",
      "          output\t\"pedestrian detection flag\"\n",
      "    refined_goal\n",
      "      objective\t\"braking\"\n",
      "      agent\t\"braking module\"\n",
      "      operation\n",
      "        operation\n",
      "          task\n",
      "            task\t\"Bring down throttle\"\n",
      "            task\t\"Apply Brakes\"\n",
      "          condition\t\"pedestrian detection flag=True\"\n",
      "          module\t\"Braking Module\"\n",
      "          input\t\"pedestrian detection flag\"\n",
      "          output\t\"Braking Status Flag\"\n",
      "        objective\t\"safe breaking\"\n",
      "  scenario\t\"A <pedestrian>  wearing a <dress> dress trying to cross road from <direction> at a distance of <distance> on a day with fog density <fog_density>\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lark import Lark\n",
    "\n",
    "# Define the grammar\n",
    "grammar = r\"\"\"\n",
    "start          : (goal)+\n",
    "goal           : temporal_op \"Achieve\" objective \"by\" refined_goal \"in scenario where\" scenario\n",
    "refined_goal   : \"Achieving\" objective \"using\" agent (\"operationalized as\" operation)? \n",
    "               | (refined_goal \"followed by\" refined_goal)* \n",
    "               | (refined_goal \"parallely\" refined_goal)*\n",
    "\n",
    "operation      : task (\"if\" condition)? \"performed by\" module \"taking input\" input (\"&\" input)* \"producing output\" output (\"&\" output)*   \n",
    "               | (operation \"triggering\" operation)* \n",
    "               | (operation \"achieving\" objective)* \n",
    "\n",
    "temporal_op    : STRING \n",
    "\n",
    "objective      : STRING\n",
    "               | objective (\"&\" objective)*\n",
    "\n",
    "condition      : STRING \n",
    "               | condition (\"&\" condition)*\n",
    "               | condition (\"|\" condition)*\n",
    "\n",
    "agent          : STRING\n",
    "task           : STRING \n",
    "               | task (\"&\" task)*\n",
    "module         : STRING\n",
    "scenario       : STRING\n",
    "input          : STRING\n",
    "output         : STRING\n",
    "\n",
    "%import common.ESCAPED_STRING   -> STRING\n",
    "%import common.WS\n",
    "%ignore WS\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate the parser\n",
    "parser = Lark(grammar, start=\"goal\")\n",
    "\n",
    "# # Define test inputs\n",
    "# examples = [\n",
    "    \n",
    "#     \"\"\"\n",
    "#     ALWAYS Achieve \"Safe Braking\" \n",
    "#     by \n",
    "#         Achieving \"Pedestrian Check\" using \"Perception Module\"\n",
    "#             operationalized as\n",
    "#                 \"capturing image\" performed by \"camera module\" taking input \"clock tick\" producing output \"image\"\n",
    "#                 triggering\n",
    "#                 \"Detect Pedestrian\" performed by \"pedestrian detection module\" \n",
    "#                 taking input \"image\" producing output \"pedestrian detection confidence\" & \"pedestrian detection flag\"\n",
    "#         parallely\n",
    "#         Achieving \"Sensing distance\" using \"Distance Estimation module\" \n",
    "#             operationalized as\n",
    "#                 \"capturing point clouds\" performed by \"Lidar Module\" taking input \"clock tick\" producing output \"point cloud\"\n",
    "#                 triggering\n",
    "#                 \"estimate pedestrian distance\" performed by \"pixel to dist mapper module\" \n",
    "#                 taking input \"image\" & \"camera properties\" & \"point cloud\" producing output \"estimated position of pedestian\" & \"distance to pedestrian\" & \"safe braking distance\"\n",
    "#         followed by \n",
    "#         Achieving \"braking\" using \"braking module\"b\n",
    "#             operationalized as \n",
    "#                 \"Bring down throttle\" & \"Apply Brakes\" if \"pedestrian detection flag=True\" & \"distance to pedestrian <= safe braking distance\" performed by \"Braking Module\" \n",
    "#                 taking input \"pedestrian detection flag\" & \"distance to pedestrian\" & \"safe braking distance\" producing output \"Braking Status Flag\" achieving \"safe breaking\"\n",
    "#     in scenario where\n",
    "#         \"A boy is trying to cross street with a dog\"\n",
    "#     \"\"\"\n",
    "# ]\n",
    "\n",
    "examples = [\n",
    "    \n",
    "    \"\"\"\n",
    "    \"ALWAYS\" Achieve \"Safe Braking\" \n",
    "    by \n",
    "        Achieving \"Pedestrian Check\" using \"Perception Module\"\n",
    "            operationalized as\n",
    "                \"capturing image\" performed by \"camera module\" taking input \"clock tick\" producing output \"image\"\n",
    "                triggering\n",
    "                \"Detect Pedestrian\" performed by \"yolov5m\" \n",
    "                taking input \"image\" producing output \"pedestrian detection confidence\" & \"pedestrian detection flag\"\n",
    "        followed by \n",
    "        Achieving \"braking\" using \"braking module\"\n",
    "            operationalized as \n",
    "                \"Bring down throttle\" & \"Apply Brakes\" if \"pedestrian detection flag=True\" performed by \"Braking Module\" \n",
    "                taking input \"pedestrian detection flag\" producing output \"Braking Status Flag\" achieving \"safe breaking\"\n",
    "    in scenario where\n",
    "        \"A <pedestrian>  wearing a <dress> dress trying to cross road from <direction> at a distance of <distance> on a day with fog density <fog_density>\"\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Parse and print the results\n",
    "for example in examples:\n",
    "    print(f\"Parsing: {example}\")\n",
    "    try:\n",
    "        parse_tree = parser.parse(example)\n",
    "        print(parse_tree.pretty())\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Token' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Extract the pedestrian detection model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfind_pedestrian_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pedestrian detection model used is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[59], line 20\u001b[0m, in \u001b[0;36mfind_pedestrian_model\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Recurse into children\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m---> 20\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfind_pedestrian_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[59], line 20\u001b[0m, in \u001b[0;36mfind_pedestrian_model\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Recurse into children\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m---> 20\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfind_pedestrian_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m, in \u001b[0;36mfind_pedestrian_model\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     17\u001b[0m                             \u001b[38;5;28;01mreturn\u001b[39;00m module_node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Recurse into children\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m:\n\u001b[1;32m     20\u001b[0m     result \u001b[38;5;241m=\u001b[39m find_pedestrian_model(child)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Token' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "from lark import Lark, Tree, Token\n",
    "\n",
    "# Recursive function to find the \"Detect Pedestrian\" task and its model\n",
    "def find_pedestrian_model(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        if tree.data == \"operation\":\n",
    "            # Check if \"Detect Pedestrian\" is a task in this operation\n",
    "            for i, child in enumerate(tree.children):\n",
    "                if isinstance(child, Tree) and child.data == \"task\":\n",
    "                    task_tokens = [str(t) for t in child.children if isinstance(t, Token)]\n",
    "                    if \"Detect Pedestrian\" in task_tokens:\n",
    "                        # Return the module (model) performing this task\n",
    "                        module_index = i + 2  # \"performed by\" is followed by the module\n",
    "                        if module_index < len(tree.children):\n",
    "                            module_node = tree.children[module_index]\n",
    "                            if isinstance(module_node, Token) and module_node.type == \"STRING\":\n",
    "                                return module_node.value\n",
    "    # Recurse into children\n",
    "    for child in tree.children:\n",
    "        result = find_pedestrian_model(child)\n",
    "        if result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "\n",
    "# Extract the pedestrian detection model\n",
    "model = find_pedestrian_model(parse_tree)\n",
    "if model:\n",
    "    print(f\"The pedestrian detection model used is: {model}\")\n",
    "else:\n",
    "    print(\"Pedestrian detection model not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perception_model(parse_tree):\n",
    "    for _st in parse_tree.iter_subtrees():\n",
    "        if isinstance(_st, Tree) and _st.data=='refined_goal':\n",
    "            for _stt in _st.children:\n",
    "                if isinstance(_stt, Tree) and _stt.data=='operation':\n",
    "                    for _sttt in _stt.children:\n",
    "                        if isinstance(_sttt, Tree)and _sttt.data=='operation':\n",
    "                            task = _sttt.children[0]\n",
    "                            module = _sttt.children[1]\n",
    "                            try:\n",
    "                                if task.children[0][1:-1] == \"Detect Pedestrian\":\n",
    "                                    return module.children[0][1:-1]\n",
    "                            except:\n",
    "                                return None\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5m'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perception_model(parse_tree=parse_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## redisAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlflow_redisai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darkaengl/Project/REAL/sim_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 15.02it/s]   \n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "The specified model does not contain any of the supported flavors for deployment. The model contains the following flavors: dict_keys(['python_function', 'pytorch']). Supported flavors: ['torchscript', 'tensorflow']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m target_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredisai://localhost:6379\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# host = localhost, port = 6379\u001b[39;00m\n\u001b[1;32m      9\u001b[0m redisai \u001b[38;5;241m=\u001b[39m get_deploy_client(target_uri)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mredisai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns:/cd13bb44eb5e43249d35846711c1f8f3/yolov5s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/REAL/sim_env/lib/python3.8/site-packages/mlflow_redisai/__init__.py:122\u001b[0m, in \u001b[0;36mRedisAIPlugin.create_deployment\u001b[0;34m(self, name, model_uri, flavor, config)\u001b[0m\n\u001b[1;32m    119\u001b[0m model_config \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mload(model_config)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flavor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     flavor \u001b[38;5;241m=\u001b[39m \u001b[43mget_preferred_deployment_flavor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     validate_deployment_flavor(model_config, flavor)\n",
      "File \u001b[0;32m~/Project/REAL/sim_env/lib/python3.8/site-packages/mlflow_redisai/utils.py:71\u001b[0m, in \u001b[0;36mget_preferred_deployment_flavor\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flavor\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     72\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified model does not contain any of the supported flavors for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m deployment. The model contains the following flavors: \u001b[39m\u001b[38;5;132;01m{model_flavors}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Supported flavors: \u001b[39m\u001b[38;5;132;01m{supported_flavors}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     76\u001b[0m                 model_flavors\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mflavors\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m     77\u001b[0m                 supported_flavors\u001b[38;5;241m=\u001b[39mSUPPORTED_DEPLOYMENT_FLAVORS)),\n\u001b[1;32m     78\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mRESOURCE_DOES_NOT_EXIST)\n",
      "\u001b[0;31mMlflowException\u001b[0m: The specified model does not contain any of the supported flavors for deployment. The model contains the following flavors: dict_keys(['python_function', 'pytorch']). Supported flavors: ['torchscript', 'tensorflow']"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"yolo models\")\n",
    "\n",
    "target_uri = 'redisai://localhost:6379'  # host = localhost, port = 6379\n",
    "redisai = get_deploy_client(target_uri)\n",
    "redisai.create_deployment('yolov5s', \"runs:/cd13bb44eb5e43249d35846711c1f8f3/yolov5s\", config={'device': 'GPU'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " 'con',\n",
       " 'create_deployment',\n",
       " 'create_endpoint',\n",
       " 'delete_deployment',\n",
       " 'delete_endpoint',\n",
       " 'explain',\n",
       " 'get_deployment',\n",
       " 'get_endpoint',\n",
       " 'list_deployments',\n",
       " 'list_endpoints',\n",
       " 'predict',\n",
       " 'predict_stream',\n",
       " 'target_uri',\n",
       " 'update_deployment',\n",
       " 'update_endpoint']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(redisai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Computing model explanations is not yet supported for this deployment target",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mredisai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/REAL/sim_env/lib/python3.8/site-packages/mlflow/deployments/base.py:252\u001b[0m, in \u001b[0;36mBaseDeploymentClient.explain\u001b[0;34m(self, deployment_name, df, endpoint)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain\u001b[39m(\u001b[38;5;28mself\u001b[39m, deployment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Generate explanations of model predictions on the specified input pandas Dataframe\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    ``df`` for the deployed model. Explanation output formats vary by deployment target,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m        an exception if the implementation is not available in deployment target's class\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing model explanations is not yet supported for this deployment target\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Computing model explanations is not yet supported for this deployment target"
     ]
    }
   ],
   "source": [
    "# redisai.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisai import Client as rai\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model_path = '/home/darkaengl/Project/REAL/model/yolov5m.pt'\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = rai(host='localhost', port=6379)\n",
    "\n",
    "# img = '/home/robb/Desktop/nn/YOLOv5_cars/yolov5/data/images/bus.jpg'\n",
    "# img = cv2.imread(img)\n",
    "\n",
    "\n",
    "# with open(model_path, 'rb') as f:\n",
    "#     model = f.read()\n",
    "\n",
    "\n",
    "# redis_client.modelstore('willitwork', 'torch', 'CPU', model)\n",
    "\n",
    "# redis_client.tensorset('my_tensor', img)\n",
    "\n",
    "# redis_client.modelexecute('willitwork', ['my_tensor'], ['output'])\n",
    "\n",
    "# output_tensor = redis_client.tensorget('output')\n",
    "# output_data = np.array(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(redis_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db': 0,\n",
       " 'username': None,\n",
       " 'password': None,\n",
       " 'socket_timeout': None,\n",
       " 'encoding': 'utf-8',\n",
       " 'encoding_errors': 'strict',\n",
       " 'decode_responses': False,\n",
       " 'retry_on_error': [],\n",
       " 'retry': None,\n",
       " 'health_check_interval': 0,\n",
       " 'client_name': None,\n",
       " 'redis_connect_func': None,\n",
       " 'credential_provider': None,\n",
       " 'host': 'localhost',\n",
       " 'port': 6379,\n",
       " 'socket_connect_timeout': None,\n",
       " 'socket_keepalive': None,\n",
       " 'socket_keepalive_options': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_client.get_connection_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Goal> ::= \"<Pattern>\" <GoalName> \":\" <Refinement> | <AgentAssignment> | <DomainConstraint>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
